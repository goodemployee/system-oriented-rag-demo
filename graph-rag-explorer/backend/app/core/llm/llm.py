# app/core/llm.py
from __future__ import annotations
from typing import List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, PreTrainedTokenizerBase, pipeline
import torch
import os

from app.capabilities.textgen.protocols import TextGenPipe
from app.capabilities.textgen.text_generator import GeneratedText


class LLM:
    """
    é€šç”¨æ–‡å­—ç”Ÿæˆæ¨¡å‹ã€‚
    å¯è¢« GraphExtractor å…±ç”¨ã€‚
    """

    def __init__(self, model_id: str, device: Optional[str] = None) -> None:
        self.model_id: str = model_id
        self.device: str = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer: Optional[PreTrainedTokenizerBase] = None
        self.model: PreTrainedModel | None = None
        self.pipe: Optional[TextGenPipe] = None

    # -------------------------------------------------------------
    # æ¨¡å‹è¼‰å…¥ / é‡‹æ”¾
    # -------------------------------------------------------------
    def load(self) -> None:
        """è¼‰å…¥ tokenizerã€æ¨¡å‹èˆ‡ç”Ÿæˆç®¡ç·š"""
        print(f"ğŸ¦™ è¼‰å…¥ LLM æ¨¡å‹ï¼š{self.model_id} ({self.device})")

        dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            torch_dtype=torch.bfloat16, 
            device_map=None,
            low_cpu_mem_usage=True
        )

        self.pipe = pipeline( # type: ignore[call-overload]
            task="text-generation",
            model=self.model,
            tokenizer=self.tokenizer, # type: ignore
            device=0 if self.device == "cuda" else -1,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.1,
            top_p=0.9,
        )

    def unload(self) -> None:
        """é‡‹æ”¾ GPU è³‡æº"""
        print("ğŸ§¹ å¸è¼‰ LLM æ¨¡å‹è³‡æº ...")
        del self.pipe, self.model, self.tokenizer
        torch.cuda.empty_cache()

    # -------------------------------------------------------------
    # æ–‡æœ¬ç”Ÿæˆæ¥å£
    # -------------------------------------------------------------
    def answer(self, question: str, passages: list[str]) -> str:
        """ç”Ÿæˆå›ç­”ï¼ˆRAG çš„ç”Ÿæˆéšæ®µï¼‰"""
        if not self.pipe:
            raise RuntimeError("LLM å°šæœªåˆå§‹åŒ–ã€‚è«‹å…ˆå‘¼å« load()ã€‚")

        context = "\n".join(passages)
        prompt = (
            f"[ç³»çµ±]\nä½ æ˜¯çŸ¥è­˜å‹åŠ©æ‰‹ï¼Œæ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œã€‚\n"
            f"[å…§å®¹]\n{context}\n"
            f"[å•é¡Œ]\n{question}\n"
            f"è«‹çµ¦å‡ºæ¸…æ™°ã€ç°¡æ½”çš„å›ç­”ï¼š"
        )

        result = self.pipe(prompt)[0]["generated_text"]
        return result.strip()
    
    def generate(self, prompt: str) -> List[GeneratedText]:
        if self.pipe is None:
            raise RuntimeError("LLM å°šæœªåˆå§‹åŒ–")

        return self.pipe(prompt)
