# app/core/embedding/embedder.py
from __future__ import annotations

import os
from pathlib import Path
from typing import Any, Optional, List, TypedDict

import torch
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.api.types import Documents, Embeddings
from app.config.paths import EMBEDDER_CACHE_DIR, CHROMA_DIR


class ChunkResult(TypedDict):
    text: str
    score: float


class Embedder:
    """
    Embedder
    ----------
    å°è£ SentenceTransformer æ¨¡å‹ + ChromaDB è³‡æ–™åº«ã€‚
    ç”¨æ–¼ï¼š
      1. æ–°å¢æ–‡æœ¬æ®µè½ï¼ˆå‘é‡åŒ–ä¸¦å…¥åº«ï¼‰
      2. æ ¹æ“š query æŸ¥è©¢æœ€ç›¸ä¼¼æ®µè½
    """

    def __init__(
        self,
        model_id: str,
        device: Optional[str] = None,
        persist_dir: Optional[str] = None,
    ) -> None:
        self.model_id: str = model_id
        self.device: str = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # --- cache è·¯å¾‘ ---
        self.cache_dir = EMBEDDER_CACHE_DIR
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # --- å‘é‡è³‡æ–™åº« ---
        persist_dir = persist_dir or str(CHROMA_DIR)
        Path(persist_dir).mkdir(parents=True, exist_ok=True)

        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection("docs")

        # --- æ¨¡å‹æœ¬é«” ---
        self.model: Optional[SentenceTransformer] = None
        print(f"ğŸ§© Embedder åˆå§‹åŒ–å®Œæˆ (model={self.model_id}, device={self.device})")

    # -------------------------------------------------------------------------
    # æ¨¡å‹è¼‰å…¥èˆ‡é‡‹æ”¾
    # -------------------------------------------------------------------------
    def load(self) -> None:
        """è¼‰å…¥ SentenceTransformer æ¨¡å‹"""
        if self.model:
            print("ğŸ” Embedder å·²è¼‰å…¥ï¼Œç•¥éã€‚")
            return

        print(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥ Embedder æ¨¡å‹ï¼š{self.model_id}")
        self.model = SentenceTransformer(
            self.model_id,
            device=self.device,
            cache_folder=str(self.cache_dir)
        )
        print("âœ… Embedder æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")

    def unload(self) -> None:
        """é‡‹æ”¾æ¨¡å‹èˆ‡ GPU è³‡æº"""
        if self.model:
            del self.model
        self.model = None
        torch.cuda.empty_cache()
        print("âœ… Embedder å·²é‡‹æ”¾ã€‚")

    # -------------------------------------------------------------------------
    # æ–‡å­—å‘é‡åŒ–
    # -------------------------------------------------------------------------
    def embed(self, texts: list[str]) -> list[list[float]]:
        """å°‡å¤šæ®µæ–‡å­—è½‰ç‚ºå‘é‡"""
        if not self.model:
            self.load()

        if self.model == None:
            raise

        embeddings = self.model.encode(
            texts,
            convert_to_numpy=True,
            show_progress_bar=False
        )
        return embeddings.tolist()

    # -------------------------------------------------------------------------
    # æ–°å¢è³‡æ–™åˆ°å‘é‡è³‡æ–™åº«
    # -------------------------------------------------------------------------
    def add_chunks(self, texts: list[str]) -> None:
        """å°‡å¤šæ®µæ–‡æœ¬å‘é‡åŒ–å¾Œå­˜å…¥ Chroma è³‡æ–™åº«"""
        if not texts:
            print("âš ï¸ add_chunks: ç©ºæ–‡æœ¬åˆ—è¡¨ï¼Œç•¥éã€‚")
            return

        if not self.model:
            self.load()

        print(f"ğŸª£ æ–°å¢ {len(texts)} ç­† chunk è‡³å‘é‡è³‡æ–™åº« ...")
        embeddings = self.embed(texts)
        ids = [f"chunk_{i}" for i in range(len(texts))]

        self.collection.add(
            documents=texts,
            embeddings=embeddings,  # type: ignore[arg-type]
            ids=ids
        )
        print("âœ… å‘é‡è³‡æ–™åº«æ–°å¢å®Œæˆã€‚")

    # -------------------------------------------------------------------------
    # æŸ¥è©¢ç›¸ä¼¼æ–‡æ®µ
    # -------------------------------------------------------------------------
    def query(self, question: str, top_k: int = 5) -> List[ChunkResult]:
        """
        æ ¹æ“šå•é¡Œæ–‡å­—ï¼ŒæŸ¥è©¢æœ€ç›¸é—œçš„æ–‡æ®µã€‚
        å›å‚³ [(text, score), ...]
        """
        if not self.model:
            self.load()

        query_vec = self.embed([question])[0]
        results = self.collection.query(
            query_embeddings=[query_vec],  # type: ignore[arg-type]
            n_results=top_k,
        )

        docs = results.get("documents", [[]])[0] # type: ignore
        scores = results.get("distances", [[]])[0] # type: ignore

        return [
            {"text": text, "score": float(score)}
            for text, score in zip(docs, scores)
        ]
