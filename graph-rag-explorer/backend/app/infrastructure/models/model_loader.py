from typing import Optional
import torch

from app.config.modules import ModulesConfig
from app.core.embedding.embedder import Embedder
from app.core.llm.llm import LLM
from app.core.graph.graph_extractor import GraphExtractor

class ModelRegistry:
    """
    çµ±ä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹å¯¦ä¾‹ï¼ˆLLM / Embedder / GraphExtractorï¼‰ã€‚
    è² è²¬è¼‰å…¥ã€å…±ç”¨ã€é‡‹æ”¾èˆ‡é¡å‹å®‰å…¨æ§åˆ¶ã€‚

    æ‰€æœ‰è·¯å¾‘ç”±å„æ¨¡å‹å…§éƒ¨é€é app.paths ç®¡ç†ã€‚
    """

    def __init__(
        self,
        modules: ModulesConfig | None = None,
        device: str | None = None,
    ) -> None:
        self.device: str = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.modules: ModulesConfig = modules or ModulesConfig()

        # æ¨¡å‹è³‡æº
        self._embedder: Optional[Embedder] = None
        self._llm: Optional[LLM] = None

    # === è¼‰å…¥æµç¨‹ ===
    ### ä¸»å‹•åˆå§‹åŒ–ä¸¦æ”¾å…¥å¿«å–
    def load_all(self) -> None:
        print(f"ğŸš€ åˆå§‹åŒ–æ¨¡å‹ (device={self.device}) ...")
        self._embedder = self.load_embedder()
        self._llm = self.load_llm()
        print("âœ… æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")

    ### æŠŠ embedder åšå¥½ä¸¦å›å‚³(å¦‚æœä¸æ¥æœƒç©ºç™¼)
    def load_embedder(self) -> Embedder:
        """Embedder é€šå¸¸æ”¾ CPU"""
        embedder  = Embedder(
            model_id=self.modules.embedder_model,
            device="cpu",
        )
        print(f"âœ… Embedder ready ({self.modules.embedder_model})")
        return embedder

    ### æŠŠ llm åšå¥½ä¸¦å›å‚³(å¦‚æœä¸æ¥æœƒç©ºç™¼)
    def load_llm(self) -> LLM:
        """è¼‰å…¥å…±ç”¨ LLMï¼Œç”¨æ–¼ç”Ÿæˆç­”æ¡ˆèˆ‡åœ–è­œæŠ½å–"""
        print("ğŸ¦™ åˆå§‹åŒ– LLM ...")
        llm = LLM(
            model_id=self.modules.llm_model,
            device=self.device,
        )
        llm.load()
        print(f"âœ… LLM ready ({self.modules.llm_model})")
        return llm

    # === é‡‹æ”¾æµç¨‹ï¼ˆå¯é¸ï¼‰ ===
    def unload_all(self) -> None:
        print("ğŸ§¹ é‡‹æ”¾æ‰€æœ‰æ¨¡å‹è³‡æº ...")

        if self._embedder and hasattr(self._embedder, "unload"):
            self._embedder.unload()

        if self._llm and hasattr(self._llm, "unload"):
            self._llm.unload()

        torch.cuda.empty_cache()
        print("âœ… è³‡æºé‡‹æ”¾å®Œç•¢")

    # === å‹åˆ¥å®‰å…¨çš„ getter ===
    ### æä¾›Embedderå¿«å–
    def _get_embedder_internal(self) -> Embedder:
        if self._embedder is None:
            self._embedder = self.load_embedder()
        
        if self._embedder is None:
            raise RuntimeError("Embedder å°šæœªè¼‰å…¥")
        
        return self._embedder

    ### æä¾›LLMå¿«å–
    def _get_llm_internal(self) -> LLM:
        if self._llm is None:
            self._llm = self.load_llm()
        
        if self._llm is None:
            raise RuntimeError("LLM å°šæœªè¼‰å…¥")
        
        return self._llm

    ### === embedderçš„å°è£ ===
    def add_chunks(self, texts: list[str]) -> None:
        embedder = self._get_embedder_internal()
        embedder.add_chunks(texts)
